
### Exercise 2: DLT Pipelines-Unity Catalog for Data governance, Metastore experience Retrieval Augmented Generation and Machine Learning

### Task 2.1: Explore Delta Live Table pipeline (Data Transformation)

Delta Live Tables (DLT) allow you to build and manage reliable data pipelines that deliver high-quality data in Lakehouse. DLT helps data engineering teams simplify ETL development and management with declarative pipeline development, automatic data testing, and deep visibility for monitoring and recovery.

1. Navigate to the **Azure Portal**, in the **rg-fabric-adb** resource group, search for **databricks** and click on the databricks resource with the name **adb-fabric...**.

![Databricks.](mediaNew/task-2.2.0new.png)

2. Click on the **Launch Workspace** button.

![Databricks.](mediaNew/task-2.2.1new.png)

3.	In the left navigation pane click on **Delta Live Table** 

![Databricks.](mediaNew/task-2.2.2new.png)

4. Click on the **Create pipeline** button.

![Databricks.](mediaNew/task-2.2.3.1new.png)

5. Enter the name of the pipeline as **DLT_Pipeline** and click on the file icon to browse the notebook.

```BASH
DLT_Pipeline
```
![Databricks.](mediaNew/task-2.2.3new.png)

6. Click on **Shared**.

7. Click on **Analytics with ADB**.

8. Click on the **01 DLT Notebook**.

9. Click on the **Select** button.

![Databricks.](mediaNew/task-2.2.4new.png)

10. Click on the **Create** button.

![Databricks.](mediaNew/task-2.2.5new.png)

>**Note**: Due to time constraints we have pre-loaded the output tables in your Databricks. We will not be executing this pipeline.

14. The result would look similar to the following screen.

![Databricks.](mediaNew/task-2.2.7.png)

---

### Task 2.2: Explore the data in Azure Databricks environment with Unity Catalog (unified governance solution for data and AI).
	
We saw how Contoso was able to utilize DLT pipelines to create a medallion architecture on their data. Now let’s look at how data governance was managed on this curated data across the organization and how it was made easy with Unity Catalog.

With the acquisition of Litware Inc., Contoso had a lot of data integration and interoperability challenges. Contoso wanted to make sure that the transition was smooth and their data engineers and data scientists could easily assimilate the data processed by Databricks. Thankfully, they had the help from a wide selection of Gen AI features right within Azure Databricks to understand and derive insights from this data. Let's see how!

**Note**: Due to time constraints, the following steps will be completed via an online Click-by-Click. Please follow the green beacons for this exercise.

1. Open the below link of click-by-click in a new tab of VM Browser.

```BASH
https://regale.cloud/Microsoft/viewer/3066/task-22-explore-the-data-in-azure-databricks-environment-with-unity-catalog/index.html#/0/0
```
2. Click on the Start Demo button.

![Databricks.](mediaNew/start-demo.png)

2. 	Follow the green beacons and expand **litware_unity_catalog db**.
   
3.	Click on **Catalog**.
5.	Expand **litware_unity_catalog db**.
6.	Expand the **rag** schema.
7.	Click on **silver_customerChurn** table.

![Databricks.](mediaNew/task-2.1new.png)

5.	Click on **Accept** in 'AI Suggested Comment' box and Click on **AI Generate**.

![Databricks.](mediaNew/task-2.1.1new.png)
	
We can see that AI in Azure Databricks has autogenerated the description for the table and its columns. Users can choose to accept the descriptions or edit them further. This improves the ease of governance on this new data for Contoso. There’s no need for Contoso’s data engineers to read through tons of documents to learn about Litware's data. How cool is that? Next, let's see how easy it is to query this data.

![Databricks.](mediaNew/task-2.2new.png)
	
6.	Select the dropdown on **Create**.

7.	Click on **Query**.

![Databricks.](mediaNew/task-2.3new.png)
	
8.	Select the **Assistant** tab.

9. Type following query in the query box: **Retrieve the average total amount of transactions for each store contract. Additionally, calculate the average total amount for customers who have churned and for those who have not churned. Ensure that all average values are rounded to the nearest whole number.**
	
![Databricks.](mediaNew/task-2.4new.png)
	
By simply using a natural language query and leveraging the AI generated table and field descriptions mentioned earlier, Azure Databricks generates an equivalent SQL query. There’s no need to be skilled in SQL queries and so business friendly, right?
	
10. Click on the **Arrow** to replace the current code.

![Databricks.](mediaNew/task-2.4.1new.png)

11.	**Run** the query.

12.	Check the output.

>**Note:** Make sure Warehouse is in ready mode

![Databricks.](mediaNew/task-2.5new.png)

Users also have the capability to fix errors in queries with the AI assistant. Let's intentionally introduce an error by misspelling a table name and see the AI's response.
	
13.	In the query, **delete** the last 2/3 letters to introduce an error.

14.	Click on **Run** to see the error.

15.	Click on **Diagnose error** to fix the query issue. 

![Databricks.](mediaNew/task-2.6new.png)
	
Data discovery is also made simple within Azure Databricks. Users can simply search for table names or the information they are looking for in the global search and all the relevant items are returned, again leveraging the table and field descriptions created by AI and data intelligence mentioned earlier.

16.	Click on **Search*.

17.	Click on **Open search in a full page**.

18. Search for **campaigns** and press enter.

![Databricks.](mediaNew/task-2.7new.png)

Click on the search box to search for Find information about customers who have churned with paperless billing and short tenure. and then click on show all results. Now, the next big challenge for Contoso was to get visibility of their Market Sentiment KPI. Remember, the Market Sentiment before the acquisition was at an all-time low. News articles and analyst reviews were being continuously published. All this unstructured data had to be efficiently assimilated so that the Market Sentiment could be tracked. That brings us to the next task. Let us see!
	
---

### Task 2.3 (OPTIONAL)Deploy LLM Chatbots With the Databricks data Intelligence Platform

Contoso also wanted to improve how efficiently they analyzed hundreds of documents and news articles about their big merger and their company policies. Why? To track and improve their Market Sentiment KPI. Azure Databricks provides just the solution with its Delta Lake architecture supporting unstructured data, like PDF documents, with Lang chain models leveraging the Databricks Foundation Model for creating custom chatbots. Let's see how this was done.

**Note**: This section is optional. Due to time constraints, the following steps will be completed via an online Click-by-Click for setting up the Unity Catalog. Please follow the green beacons and the instructions on the screen for this exercise..

```BASH
https://regale.cloud/Microsoft/viewer/3067/task-23-deploy-llm-chatbots-with-the-data-intelligence-platform/index.html#/0/0
```
1. Click on the Start Demo button.

![Databricks.](mediaNew/start-demo.png)

