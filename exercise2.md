
### Exercise 2: Unity Catalog (data governance), Metastore experience, RAG and ML

### Task 2.1: Delta Live Table pipeline (Interactive)

1. Navigate to the **Azure Portal**, in the **rg-fabric-adb** resource group, search for **databricks** and click on the databricks resource with the name **adb-fabric...**.

![Databricks.](mediaNew/task-2.2.0new.png)

2. Click on the **Launch Workspace** button.

![Databricks.](mediaNew/task-2.2.1new.png)

3.	In the left navigation pane click on **Delta Live Table** 

![Databricks.](mediaNew/task-2.2.2new.png)

4. Click on the **Create pipeline** button.

![Databricks.](mediaNew/task-2.2.3.1new.png)

5. Enter the name of the pipeline as **DLT_Pipeline** and click on the file icon to browse the notebook.

```BASH
DLT_Pipeline
```
![Databricks.](mediaNew/task-2.2.3new.png)

6. Click on **Shared**.

7. Click on **Analytics with ADB**.

8. Click on the **01 DLT Notebook**.

9. Click on the **Select** button.

![Databricks.](mediaNew/task-2.2.4new.png)

10. Click on the **Create** button.

![Databricks.](mediaNew/task-2.2.5new.png)

>**Note**: Due to time constraints we have pre-loaded the output tables in your Databricks. We will not be executing this pipeline.

14. The result would look similar to the following screen.

![Databricks.](mediaNew/task-2.2.7.png)

---

### Task 2.2: Explore the data in Azure Databricks environment with Unity Catalog (unified governance solution for data and AI).
	
With the acquisition of Litware Inc., Contoso had a lot of data integration and interoperability challenges. Contoso wanted to make sure that the transition was smooth and data engineers and scientists from Contoso could easily assimilate the data processed by Databricks. Thankfully, they had the help from Gen AI features right within Azure Databricks to understand and derive insights from this data. Let's see how!
	
1.	Click on **Catalog**.

2.	Expand **litware_unity_catalog db**.

3.	Expand the **rag** schema.

4.	Click on **silver_customerChurn** table.

![Databricks.](mediaNew/task-2.1new.png)

5.	Click on **Accept** in 'AI Suggested Comment' box and Click on **AI Generate**.

![Databricks.](mediaNew/task-2.1.1new.png)
	
We can see that Databricks has autogenerated the description for the table and its columns. Users can choose to accept the descriptions or edit them further. This improves the ease of governance on this new data for Contoso. Next, let's see how easy it is to query this data.

![Databricks.](mediaNew/task-2.2new.png)
	
6.	Select the dropdown on **Create**.

7.	Click on **Query**.

![Databricks.](mediaNew/task-2.3new.png)
	
8.	Select the **Assistant** tab.

9.	Type following query in the query box: **Retrieve the average total amount of transactions for each store contract. Additionally, calculate the average total amount for customers who have churned and for those who have not churned. Ensure that all average values are rounded to the nearest whole number.**
	
![Databricks.](mediaNew/task-2.4new.png)
	
By simply using a natural language query, databricks generates an equivalet SQL query.
	
10. Click on the **Arrow** to replace the current code.

![Databricks.](mediaNew/task-2.4.1new.png)

11.	**Run** the query.

12.	Check the output.

>**Note:** Make sure Warehouse is in ready mode

![Databricks.](mediaNew/task-2.5new.png)

Users also have the capability to fix errors in queries with the AI assistant. 
	
13.	In the query, **delete** the last 2/3 letters to introduce an error.

14.	Click on **Run** to see the error.

15.	Click on **Diagnose error** to fix the query issue. 

![Databricks.](mediaNew/task-2.6new.png)
	
Data discovery is also made simple within Azure Databricks. Users can simply search for table names or the information they are looking for in the global search and all the relevant items are returned.
	
16.	Click on **Search*.

17.	Click on **Open search in a full page**.

18. Search for **campaigns** and press enter.

![Databricks.](mediaNew/task-2.7new.png)
	
---

### Task 2.3 (OPTIONAL)Deploy LLM Chatbots With the Data Intelligence Platform 

Contoso also wanted to improve their efficientcy with analyzing hundreds of documents about their big merger and their company policies. Azure Databricks provides just the solution with its Delta Lake architecture supporting unstructured data, like PDF documents, with Lang chain models leveraging Databricks Foundation Model for creating custom chatbots. Let's see how this was done.

![Databricks.](mediaNew/task-2.3.1.png)

First, let's ingest our PDFs as a Delta Lake table with path URLs and content in binary format.

1.	Go to **Unity Catalog Volumes**.

2.	Click on **documents_store**.

![Databricks.](mediaNew/task-2.3.1.a.new.png)

3.	Click on **pdf_documents**.

4.	Review the Knowledge Base (pdfs).

![Databricks.](mediaNew/task-2.3.2new.png)

We'll use Databricks Autoloader to incrementally ingeset new files, making it easy to incrementally consume large volume of files from the data lake in various data formats. Autoloader easily ingests our unstructured PDF data in binary format.

5. Click on **Workspaces**

![Databricks.](mediaNew/task-2.3.3new.png)

6. Open the notebook from the path shown in the screenshot below.

![Databricks.](mediaNew/task-2.3.3.a.new.png)

This notebook is used to convert the ingested document into delta tables. Lets look at the output tables.

7. Click on **Catalog** and under **rag** database, click on the **documents_raw** table.

![Databricks.](mediaNew/task-2.3.3.b.new.png)

8.	Click on **Sample Data**.

9.	Review the delta table.

![Databricks.](mediaNew/task-2.3.4new.png)

Next we convert the PDF documents bytes to text, extract chunks from their content, and create a vector search index for retreival.

10.	Go to table: **documents_embedding**.

![Databricks.](mediaNew/task-2.3.4.a.new.png)

11.	Click **Sample Data**.

12.	Review the Delta Table.

![Databricks.](mediaNew/task-2.3.4.b.new.png)

13.	In the upper-right corner, click on the dropdown for **Create**.

14.	Select **Vector search index**. Click on **Cancel** after reviewing the fields.

![Databricks.](mediaNew/task-2.3.6new.png)

We've just seen how Databricks Lakehouse AI makes it easy to ingest and prepare your documents and deploy a Self-Managed Vector Search index on top of it with just a few lines of code and configuration.

15. Click on **Workspace**.

16. Click on **Shared**.

17. Click on **RetrievalAugmentedGeneration**.

![Databricks.](mediaNew/task-2.3.7.a.new.png)

18. Click on notebook **3. Register and Deploy RAG model as Endpoint**.

![Databricks.](mediaNew/task-2.3.7.new.png)

With this model, we will be able to serve and accept questions based on the documents uploaded.
Everytime we send a question to the chatbot, the following steps occur:
•	The model receiv  es the question
•	The retriever automatically fetches related chunks from our documents
•	A prompt is crafted with the chunks and the question
•	The prompt is sent to the Databricks Foundation Model and Llama 2 serves an accurate answer based on the chunk information!

You will witness this in action [here](https://app-ms-build-2024-demo.azurewebsites.net/#/enterprise-chatbot).

---
